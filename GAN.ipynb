{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d55bce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n",
      "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "763           10      101             76             48      180  32.9   \n",
      "764            2      122             70             27        0  36.8   \n",
      "765            5      121             72             23      112  26.2   \n",
      "766            1      126             60              0        0  30.1   \n",
      "767            1       93             70             31        0  30.4   \n",
      "\n",
      "     DiabetesPedigreeFunction  Age  Outcome  \n",
      "763                     0.171   63        0  \n",
      "764                     0.340   27        0  \n",
      "765                     0.245   30        0  \n",
      "766                     0.349   47        1  \n",
      "767                     0.315   23        0  \n",
      "Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
      "       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saubh\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Accuracy: 0.7489177489177489\n",
      "Base classification report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.81       151\n",
      "           1       0.64      0.62      0.63        80\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.72      0.72      0.72       231\n",
      "weighted avg       0.75      0.75      0.75       231\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 15)                165       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 30)                480       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 9)                 279       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 924 (3.61 KB)\n",
      "Trainable params: 924 (3.61 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 25)                250       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 50)                1300      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1601 (6.25 KB)\n",
      "Trainable params: 1601 (6.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from numpy.random import randn\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "data = pd.read_csv('G:\\Rakuten\\diabetes.csv')\n",
    "print (data.shape)\n",
    "print (data.tail())\n",
    "print (data.columns)\n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "label = ['Outcome']\n",
    "X = data[features]\n",
    "y = data[label] \n",
    "X_true_train, X_true_test, y_true_train, y_true_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "clf_true = RandomForestClassifier(n_estimators=100)\n",
    "clf_true.fit(X_true_train,y_true_train)\n",
    "y_true_pred=clf_true.predict(X_true_test)\n",
    "print(\"Base Accuracy:\",metrics.accuracy_score(y_true_test, y_true_pred))\n",
    "print(\"Base classification report:\",metrics.classification_report(y_true_test, y_true_pred))\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    X = generator.predict(x_input)\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def generate_real_samples(n):\n",
    "    X = data.sample(n)\n",
    "    y = np.ones((n, 1))\n",
    "    return X, y\n",
    "def define_generator(latent_dim, n_outputs=9):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, activation='relu',  kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='linear'))\n",
    "    return model\n",
    "generator1 = define_generator(10, 9)\n",
    "generator1.summary()\n",
    "def define_discriminator(n_inputs=9):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "discriminator1 = define_discriminator(9)\n",
    "discriminator1.summary()\n",
    "\n",
    "def define_gan(generator, discriminator):\n",
    "    # make weights in the discriminator not trainable\n",
    "    discriminator.trainable = False\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(generator)\n",
    "    # add the discriminator\n",
    "    model.add(discriminator)\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def plot_history(d_hist, g_hist):\n",
    "    # plot loss\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.plot(d_hist, label='d')\n",
    "    plt.plot(g_hist, label='gen')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def train(g_model, d_model, gan_model, latent_dim, n_epochs=10000, n_batch=128, n_eval=200):\n",
    "    \n",
    "    half_batch = int(n_batch / 2)\n",
    "    d_history = []\n",
    "    g_history = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "    \n",
    "        \n",
    "        x_real, y_real = generate_real_samples(half_batch)\n",
    "        # prepare fake examples\n",
    "        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "        # update discriminator\n",
    "        d_loss_real, d_real_acc = d_model.train_on_batch(x_real, y_real)\n",
    "        d_loss_fake, d_fake_acc = d_model.train_on_batch(x_fake, y_fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        # prepare points in latent space as input for the generator\n",
    "        x_gan = generate_latent_points(latent_dim, n_batch)\n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = np.ones((n_batch, 1))\n",
    "        # update the generator via the discriminator's error\n",
    "        g_loss_fake = gan_model.train_on_batch(x_gan, y_gan)\n",
    "        print('>%d, d1=%.3f, d2=%.3f d=%.3f g=%.3f' % (epoch+1, d_loss_real, d_loss_fake, d_loss,  g_loss_fake))\n",
    "        d_history.append(d_loss)\n",
    "        g_history.append(g_loss_fake)\n",
    "        plot_history(d_history, g_history)\n",
    "        g_model.save('trained_generated_model.h5')\n",
    "# size of the latent space\n",
    "latent_dim = 10\n",
    "\n",
    "discriminator = define_discriminator()\n",
    "\n",
    "generator = define_generator(latent_dim)\n",
    "\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# train model\n",
    "train(generator, discriminator, gan_model, latent_dim)\n",
    "from keras.models import load_model\n",
    "model =load_model('trained_generated_model.h5',compile=False)\n",
    "latent_points = generate_latent_points(10, 750)\n",
    "X = model.predict(latent_points)\n",
    "data_fake = pd.DataFrame(data=X,  columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'])\n",
    "data_fake.head()\n",
    "outcome_mean = data_fake.Outcome.mean()\n",
    "data_fake['Outcome'] = data_fake['Outcome'] > outcome_mean\n",
    "data_fake[\"Outcome\"] = data_fake[\"Outcome\"].astype(int)\n",
    "features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "label = ['Outcome']\n",
    "X_fake_created = data_fake[features]\n",
    "y_fake_created = data_fake[label]\n",
    "X_fake_train, X_fake_test, y_fake_train, y_fake_test = train_test_split(X_fake_created, y_fake_created, test_size=0.30, random_state=42)\n",
    "clf_fake = RandomForestClassifier(n_estimators=100)\n",
    "clf_fake.fit(X_fake_train,y_fake_train)\n",
    "y_fake_pred=clf_fake.predict(X_fake_test)\n",
    "print(\"Accuracy of fake data model:\",metrics.accuracy_score(y_fake_test, y_fake_pred))\n",
    "print(\"Classification report of fake data model:\",metrics.classification_report(y_fake_test, y_fake_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
